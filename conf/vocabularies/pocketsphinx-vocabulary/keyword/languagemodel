#############################################################################
## Copyright (c) 1996, Carnegie Mellon University, Cambridge University,
## Ronald Rosenfeld and Philip Clarkson
## Version 3, Copyright (c) 2006, Carnegie Mellon University 
## Contributors includes Wen Xu, Ananlada Chotimongkol, 
## David Huggins-Daines, Arthur Chan and Alan Black 
#############################################################################
=============================================================================
===============  This file was produced by the CMU-Cambridge  ===============
===============     Statistical Language Modeling Toolkit     ===============
=============================================================================
This is a 3-gram language model, based on a vocabulary of 13 words,
  which begins "</s>", "<s>", "BE"...
This is an OPEN-vocabulary model (type 1)
  (OOVs were mapped to UNK, which is treated as any other vocabulary word)
Good-Turing discounting was applied.
1-gram frequency of frequency : 9 
2-gram frequency of frequency : 20 0 0 0 0 0 0 
3-gram frequency of frequency : 30 0 0 0 0 0 0 
1-gram discounting ratios : 0.90 
2-gram discounting ratios : 
3-gram discounting ratios : 
This file is in the ARPA-standard format introduced by Doug Paul.

p(wd3|wd1,wd2)= if(trigram exists)           p_3(wd1,wd2,wd3)
                else if(bigram w1,w2 exists) bo_wt_2(w1,w2)*p(wd3|wd2)
                else                         p(wd3|w2)

p(wd2|wd1)= if(bigram exists) p_2(wd1,wd2)
            else              bo_wt_1(wd1)*p_1(wd2)

All probs and back-off weights (bo_wt) are given in log10 form.

Data formats:

Beginning of data mark: \data\
ngram 1=nr            # number of 1-grams
ngram 2=nr            # number of 2-grams
ngram 3=nr            # number of 3-grams

\1-grams:
p_1     wd_1 bo_wt_1
\2-grams:
p_2     wd_1 wd_2 bo_wt_2
\3-grams:
p_3     wd_1 wd_2 wd_3 

end of data mark: \end\

\data\
ngram 1=14
ngram 2=24
ngram 3=34

\1-grams:
-1.8909 <UNK>	0.0000
-0.5441 </s>	-1.1875
-0.5229 <s>	-1.2956
-1.8909 BE	-0.3310
-1.8909 DID	-0.3310
-1.8909 FIRST	-0.3310
-1.8909 IN	-0.3310
-1.8909 JASPER	-0.3310
-1.8909 NOW	-0.3310
-0.8451 OKEY	-0.8709
-1.8909 RIGHT	-0.3310
-1.8909 SAY	-0.3310
-0.8451 TOMMY	-0.9331
-1.8909 WHAT	-0.3310

\2-grams:
-0.0202 </s> <s> -0.2808
-1.2175 <s> BE 0.1761
-1.2175 <s> DID 0.1761
-1.2175 <s> FIRST 0.1761
-1.2175 <s> IN 0.1761
-1.2175 <s> JASPER 0.1761
-1.2175 <s> NOW 0.1761
-0.4393 <s> OKEY 0.0348
-1.2175 <s> RIGHT 0.1761
-1.2175 <s> SAY 0.1761
-1.2175 <s> TOMMY 0.7782
-1.2175 <s> WHAT 0.1761
-0.1761 BE </s> 1.0414
-0.1761 DID </s> 1.0414
-0.1761 FIRST </s> 1.0414
-0.1761 IN </s> 1.0414
-0.1761 JASPER </s> 1.0414
-0.1761 NOW </s> 1.0414
-0.8129 OKEY </s> 1.0414
-0.1139 OKEY TOMMY 0.0792
-0.1761 RIGHT </s> 1.0414
-0.1761 SAY </s> 1.0414
-0.0378 TOMMY </s> 0.3010
-0.1761 WHAT </s> 1.0414

\3-grams:
-1.3222 </s> <s> DID 
-1.3222 </s> <s> FIRST 
-1.3222 </s> <s> IN 
-1.3222 </s> <s> JASPER 
-1.3222 </s> <s> NOW 
-0.2808 </s> <s> OKEY 
-1.3222 </s> <s> RIGHT 
-1.3222 </s> <s> SAY 
-1.3222 </s> <s> TOMMY 
-1.3222 </s> <s> WHAT 
-0.3010 <s> BE </s> 
-0.3010 <s> DID </s> 
-0.3010 <s> FIRST </s> 
-0.3010 <s> IN </s> 
-0.3010 <s> JASPER </s> 
-0.3010 <s> NOW </s> 
-1.0792 <s> OKEY </s> 
-0.0792 <s> OKEY TOMMY 
-0.3010 <s> RIGHT </s> 
-0.3010 <s> SAY </s> 
-0.3010 <s> TOMMY </s> 
-0.3010 <s> WHAT </s> 
-0.3010 BE </s> <s> 
-0.3010 DID </s> <s> 
-0.3010 FIRST </s> <s> 
-0.3010 IN </s> <s> 
-0.3010 JASPER </s> <s> 
-0.3010 NOW </s> <s> 
-0.3010 OKEY </s> <s> 
-0.0458 OKEY TOMMY </s> 
-0.3010 RIGHT </s> <s> 
-0.3010 SAY </s> <s> 
-0.0414 TOMMY </s> <s> 
-0.3010 WHAT </s> <s> 

\end\
